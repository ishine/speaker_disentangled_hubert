<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        :root {
            --c0: #dedede;
        }

        td {
            vertical-align: middle;
        }

        audio {
            width: 20vw;
            min-width: 100px;
            max-width: 250px;
        }

        .float-div {
            box-shadow: 0 0.5rem 1rem var(--c0);
            padding: 3rem;
            margin-top: 3rem;
            margin-bottom: 3rem;
            border-radius: 0.5rem;
            background: white;
        }

        p {
            text-align: justify;
        }
    </style>
</head>

<div class="container float-div">
    <div class="text-center">
        <h3>Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT</h3>
        <h6>
            [<a href="https://arxiv.org/abs/2409.10103" , target='_blank'>Paper</a>]
            [<a href="https://github.com/ryota-komatsu/speaker_disentangled_hubert" , target='_blank'>Code</a>]
            [<a href="https://huggingface.co/ryota-komatsu/s5-hubert" , target='_blank'>Model</a>]
            [<a href="https://huggingface.co/datasets/ryota-komatsu/libritts-r-s5-hubert-8192units" , target='_blank'>Dataset</a>]
        </h6>
    </div>

    <p style="text-align: center; font-style: italic;">
        Ryota Komatsu<sup>1</sup>, Takahiro Shinozaki<sup>1</sup>
    </p>
    <p style="text-align: center;">
        <sup>1</sup>Institute of Science Tokyo
    </p>
</div>

<div class="container float-div">
    <div class="text-center">
        <h3>Speech resynthesis samples from LibriTTS-R test set</h3>
    </div>
    <div class="table-responsive pt-3">
        <table style="text-align: center" class="table table-hover">
            <thead>
                <tr>
                    <th style="text-align: center; vertical-align: top" rowspan="2">Original</th>
                    <th style="text-align: center" colspan="2">Resynthesis</th>
                </tr>
                <tr>
                    <th style="text-align: center">sampling #1</th>
                    <th style="text-align: center">sampling #2</th>
                </tr>
            </thead>
            <tbody>

                <tr>
                    <td><audio controls controlslist="nodownload" src='audio/'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/'></audio></td>
                </tr>

            </tbody>
        </table>
    </div>
</div>

<div class="container">
    <h3>License</h3>
    <p>
        The
        <a href="https://www.openslr.org/141/" , target='_blank'>LibriTTS-R</a>
        dataset is made available by Google LLC under the
        <a href="http://creativecommons.org/licenses/by/4.0/" , target='_blank'>CC BY 4.0</a>.
    </p>
</div>

<div class="container">
    <h3>References</h3>
    <ol>
        <li>Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka, M. Bacchiani, Y. Zhang, W. Han, and A. Bapna, "LibriTTS-R: A restored multi-speaker text-to-speech corpus," in Proc. Interspeech, 2023, pp. 5496–5500.</li>
        <li>M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, and W.-N. Hsu, "Voicebox: Text-guided multilingual universal speech generation at scale," in Proc. Thirty-seventh Conference on Neural Information Processing Systems, vol. 36, 2023, pp. 14005–14034.</li>
        <li>S. gil Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, "BigVGAN: A universal neural vocoder with large-scale training," in Proc. International Conference on Learning Representations, 2023.</li>
    </ol>
</div>

</html>